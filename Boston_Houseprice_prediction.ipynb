{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPePH7lcJ8doGf4mwcjtFkq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Boston House Price Prediction"
      ],
      "metadata": {
        "id": "t5Npz4Apw7CB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "22WU-kcdw50G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "housing = fetch_california_housing()"
      ],
      "metadata": {
        "id": "ciikg1vsxwgN"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x , y = housing.data , housing.target\n",
        "# x = input feature\n",
        "# y= targets"
      ],
      "metadata": {
        "id": "6FFaqnhhsw-g"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.from_numpy(x).float()\n",
        "target = torch.from_numpy(y).float()"
      ],
      "metadata": {
        "id": "v7XEBHwUszhG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input.shape)\n",
        "print(target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGmCQxSIuN5s",
        "outputId": "e68edacd-7de7-4446-8b4e-90c13d072b86"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20640, 8])\n",
            "torch.Size([20640])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.randn(8,1, requires_grad = True)\n",
        "b = torch.randn(1, requires_grad = True)"
      ],
      "metadata": {
        "id": "EG17ZsfZw6YB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0tkMFtTxgen",
        "outputId": "d81d86cd-46fe-4dd3-8bd3-39a673f25f7e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0911],\n",
            "        [ 0.8049],\n",
            "        [ 2.2662],\n",
            "        [ 0.7773],\n",
            "        [ 1.4298],\n",
            "        [ 1.1149],\n",
            "        [ 0.0280],\n",
            "        [-0.1252]], requires_grad=True)\n",
            "tensor([-0.6811], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.randn(8,1, requires_grad = True)\n",
        "b = torch.randn(1, requires_grad = True)\n",
        "def model(x):\n",
        "    return x @ w + b\n",
        "\n",
        "def mse(pred, targ):\n",
        "    diff = pred - targ\n",
        "    return torch.sum(diff * diff) / diff.numel()\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 300\n",
        "lr = 0.0000001\n",
        "\n",
        "for i in range(epochs):\n",
        "    prediction = model(input)\n",
        "    loss = mse(prediction, target)\n",
        "\n",
        "    # Compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights and biases\n",
        "    with torch.no_grad():\n",
        "        w -= w.grad * lr\n",
        "        b -= b.grad * lr\n",
        "        # Zero the gradients\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()\n",
        "\n",
        "    print(f\"Epoch {i+1}: Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSYjt8ch7hmk",
        "outputId": "d5f031e1-cade-4c7a-d15d-5b48344680ed"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss: 13238168.0000\n",
            "Epoch 2: Loss: 1487291.2500\n",
            "Epoch 3: Loss: 167546.3438\n",
            "Epoch 4: Loss: 19324.4219\n",
            "Epoch 5: Loss: 2676.7314\n",
            "Epoch 6: Loss: 806.1604\n",
            "Epoch 7: Loss: 595.2094\n",
            "Epoch 8: Loss: 570.6531\n",
            "Epoch 9: Loss: 567.0332\n",
            "Epoch 10: Loss: 565.7668\n",
            "Epoch 11: Loss: 564.7669\n",
            "Epoch 12: Loss: 563.7991\n",
            "Epoch 13: Loss: 562.8373\n",
            "Epoch 14: Loss: 561.8783\n",
            "Epoch 15: Loss: 560.9217\n",
            "Epoch 16: Loss: 559.9677\n",
            "Epoch 17: Loss: 559.0161\n",
            "Epoch 18: Loss: 558.0669\n",
            "Epoch 19: Loss: 557.1202\n",
            "Epoch 20: Loss: 556.1759\n",
            "Epoch 21: Loss: 555.2342\n",
            "Epoch 22: Loss: 554.2947\n",
            "Epoch 23: Loss: 553.3577\n",
            "Epoch 24: Loss: 552.4231\n",
            "Epoch 25: Loss: 551.4908\n",
            "Epoch 26: Loss: 550.5610\n",
            "Epoch 27: Loss: 549.6336\n",
            "Epoch 28: Loss: 548.7086\n",
            "Epoch 29: Loss: 547.7859\n",
            "Epoch 30: Loss: 546.8656\n",
            "Epoch 31: Loss: 545.9478\n",
            "Epoch 32: Loss: 545.0322\n",
            "Epoch 33: Loss: 544.1190\n",
            "Epoch 34: Loss: 543.2081\n",
            "Epoch 35: Loss: 542.2996\n",
            "Epoch 36: Loss: 541.3934\n",
            "Epoch 37: Loss: 540.4894\n",
            "Epoch 38: Loss: 539.5880\n",
            "Epoch 39: Loss: 538.6888\n",
            "Epoch 40: Loss: 537.7919\n",
            "Epoch 41: Loss: 536.8973\n",
            "Epoch 42: Loss: 536.0051\n",
            "Epoch 43: Loss: 535.1151\n",
            "Epoch 44: Loss: 534.2275\n",
            "Epoch 45: Loss: 533.3420\n",
            "Epoch 46: Loss: 532.4588\n",
            "Epoch 47: Loss: 531.5779\n",
            "Epoch 48: Loss: 530.6993\n",
            "Epoch 49: Loss: 529.8229\n",
            "Epoch 50: Loss: 528.9487\n",
            "Epoch 51: Loss: 528.0770\n",
            "Epoch 52: Loss: 527.2073\n",
            "Epoch 53: Loss: 526.3400\n",
            "Epoch 54: Loss: 525.4748\n",
            "Epoch 55: Loss: 524.6118\n",
            "Epoch 56: Loss: 523.7510\n",
            "Epoch 57: Loss: 522.8926\n",
            "Epoch 58: Loss: 522.0363\n",
            "Epoch 59: Loss: 521.1822\n",
            "Epoch 60: Loss: 520.3303\n",
            "Epoch 61: Loss: 519.4807\n",
            "Epoch 62: Loss: 518.6331\n",
            "Epoch 63: Loss: 517.7878\n",
            "Epoch 64: Loss: 516.9445\n",
            "Epoch 65: Loss: 516.1035\n",
            "Epoch 66: Loss: 515.2647\n",
            "Epoch 67: Loss: 514.4279\n",
            "Epoch 68: Loss: 513.5934\n",
            "Epoch 69: Loss: 512.7611\n",
            "Epoch 70: Loss: 511.9307\n",
            "Epoch 71: Loss: 511.1025\n",
            "Epoch 72: Loss: 510.2764\n",
            "Epoch 73: Loss: 509.4526\n",
            "Epoch 74: Loss: 508.6308\n",
            "Epoch 75: Loss: 507.8112\n",
            "Epoch 76: Loss: 506.9937\n",
            "Epoch 77: Loss: 506.1782\n",
            "Epoch 78: Loss: 505.3648\n",
            "Epoch 79: Loss: 504.5535\n",
            "Epoch 80: Loss: 503.7443\n",
            "Epoch 81: Loss: 502.9372\n",
            "Epoch 82: Loss: 502.1321\n",
            "Epoch 83: Loss: 501.3293\n",
            "Epoch 84: Loss: 500.5282\n",
            "Epoch 85: Loss: 499.7293\n",
            "Epoch 86: Loss: 498.9325\n",
            "Epoch 87: Loss: 498.1378\n",
            "Epoch 88: Loss: 497.3450\n",
            "Epoch 89: Loss: 496.5544\n",
            "Epoch 90: Loss: 495.7658\n",
            "Epoch 91: Loss: 494.9791\n",
            "Epoch 92: Loss: 494.1945\n",
            "Epoch 93: Loss: 493.4119\n",
            "Epoch 94: Loss: 492.6313\n",
            "Epoch 95: Loss: 491.8527\n",
            "Epoch 96: Loss: 491.0761\n",
            "Epoch 97: Loss: 490.3015\n",
            "Epoch 98: Loss: 489.5289\n",
            "Epoch 99: Loss: 488.7582\n",
            "Epoch 100: Loss: 487.9896\n",
            "Epoch 101: Loss: 487.2229\n",
            "Epoch 102: Loss: 486.4582\n",
            "Epoch 103: Loss: 485.6955\n",
            "Epoch 104: Loss: 484.9347\n",
            "Epoch 105: Loss: 484.1758\n",
            "Epoch 106: Loss: 483.4189\n",
            "Epoch 107: Loss: 482.6640\n",
            "Epoch 108: Loss: 481.9110\n",
            "Epoch 109: Loss: 481.1600\n",
            "Epoch 110: Loss: 480.4107\n",
            "Epoch 111: Loss: 479.6635\n",
            "Epoch 112: Loss: 478.9182\n",
            "Epoch 113: Loss: 478.1748\n",
            "Epoch 114: Loss: 477.4333\n",
            "Epoch 115: Loss: 476.6936\n",
            "Epoch 116: Loss: 475.9560\n",
            "Epoch 117: Loss: 475.2201\n",
            "Epoch 118: Loss: 474.4863\n",
            "Epoch 119: Loss: 473.7543\n",
            "Epoch 120: Loss: 473.0241\n",
            "Epoch 121: Loss: 472.2957\n",
            "Epoch 122: Loss: 471.5694\n",
            "Epoch 123: Loss: 470.8447\n",
            "Epoch 124: Loss: 470.1221\n",
            "Epoch 125: Loss: 469.4012\n",
            "Epoch 126: Loss: 468.6822\n",
            "Epoch 127: Loss: 467.9650\n",
            "Epoch 128: Loss: 467.2497\n",
            "Epoch 129: Loss: 466.5363\n",
            "Epoch 130: Loss: 465.8246\n",
            "Epoch 131: Loss: 465.1147\n",
            "Epoch 132: Loss: 464.4067\n",
            "Epoch 133: Loss: 463.7005\n",
            "Epoch 134: Loss: 462.9962\n",
            "Epoch 135: Loss: 462.2935\n",
            "Epoch 136: Loss: 461.5927\n",
            "Epoch 137: Loss: 460.8937\n",
            "Epoch 138: Loss: 460.1965\n",
            "Epoch 139: Loss: 459.5011\n",
            "Epoch 140: Loss: 458.8075\n",
            "Epoch 141: Loss: 458.1156\n",
            "Epoch 142: Loss: 457.4255\n",
            "Epoch 143: Loss: 456.7372\n",
            "Epoch 144: Loss: 456.0508\n",
            "Epoch 145: Loss: 455.3659\n",
            "Epoch 146: Loss: 454.6829\n",
            "Epoch 147: Loss: 454.0016\n",
            "Epoch 148: Loss: 453.3221\n",
            "Epoch 149: Loss: 452.6443\n",
            "Epoch 150: Loss: 451.9682\n",
            "Epoch 151: Loss: 451.2939\n",
            "Epoch 152: Loss: 450.6213\n",
            "Epoch 153: Loss: 449.9505\n",
            "Epoch 154: Loss: 449.2812\n",
            "Epoch 155: Loss: 448.6138\n",
            "Epoch 156: Loss: 447.9481\n",
            "Epoch 157: Loss: 447.2841\n",
            "Epoch 158: Loss: 446.6217\n",
            "Epoch 159: Loss: 445.9611\n",
            "Epoch 160: Loss: 445.3021\n",
            "Epoch 161: Loss: 444.6449\n",
            "Epoch 162: Loss: 443.9893\n",
            "Epoch 163: Loss: 443.3354\n",
            "Epoch 164: Loss: 442.6832\n",
            "Epoch 165: Loss: 442.0326\n",
            "Epoch 166: Loss: 441.3838\n",
            "Epoch 167: Loss: 440.7365\n",
            "Epoch 168: Loss: 440.0910\n",
            "Epoch 169: Loss: 439.4471\n",
            "Epoch 170: Loss: 438.8049\n",
            "Epoch 171: Loss: 438.1642\n",
            "Epoch 172: Loss: 437.5252\n",
            "Epoch 173: Loss: 436.8878\n",
            "Epoch 174: Loss: 436.2522\n",
            "Epoch 175: Loss: 435.6180\n",
            "Epoch 176: Loss: 434.9856\n",
            "Epoch 177: Loss: 434.3547\n",
            "Epoch 178: Loss: 433.7255\n",
            "Epoch 179: Loss: 433.0978\n",
            "Epoch 180: Loss: 432.4718\n",
            "Epoch 181: Loss: 431.8474\n",
            "Epoch 182: Loss: 431.2245\n",
            "Epoch 183: Loss: 430.6033\n",
            "Epoch 184: Loss: 429.9837\n",
            "Epoch 185: Loss: 429.3657\n",
            "Epoch 186: Loss: 428.7492\n",
            "Epoch 187: Loss: 428.1342\n",
            "Epoch 188: Loss: 427.5210\n",
            "Epoch 189: Loss: 426.9092\n",
            "Epoch 190: Loss: 426.2990\n",
            "Epoch 191: Loss: 425.6904\n",
            "Epoch 192: Loss: 425.0833\n",
            "Epoch 193: Loss: 424.4778\n",
            "Epoch 194: Loss: 423.8738\n",
            "Epoch 195: Loss: 423.2714\n",
            "Epoch 196: Loss: 422.6705\n",
            "Epoch 197: Loss: 422.0712\n",
            "Epoch 198: Loss: 421.4734\n",
            "Epoch 199: Loss: 420.8771\n",
            "Epoch 200: Loss: 420.2823\n",
            "Epoch 201: Loss: 419.6891\n",
            "Epoch 202: Loss: 419.0974\n",
            "Epoch 203: Loss: 418.5072\n",
            "Epoch 204: Loss: 417.9185\n",
            "Epoch 205: Loss: 417.3313\n",
            "Epoch 206: Loss: 416.7456\n",
            "Epoch 207: Loss: 416.1614\n",
            "Epoch 208: Loss: 415.5787\n",
            "Epoch 209: Loss: 414.9975\n",
            "Epoch 210: Loss: 414.4177\n",
            "Epoch 211: Loss: 413.8395\n",
            "Epoch 212: Loss: 413.2628\n",
            "Epoch 213: Loss: 412.6874\n",
            "Epoch 214: Loss: 412.1136\n",
            "Epoch 215: Loss: 411.5413\n",
            "Epoch 216: Loss: 410.9704\n",
            "Epoch 217: Loss: 410.4008\n",
            "Epoch 218: Loss: 409.8329\n",
            "Epoch 219: Loss: 409.2664\n",
            "Epoch 220: Loss: 408.7012\n",
            "Epoch 221: Loss: 408.1376\n",
            "Epoch 222: Loss: 407.5754\n",
            "Epoch 223: Loss: 407.0146\n",
            "Epoch 224: Loss: 406.4553\n",
            "Epoch 225: Loss: 405.8974\n",
            "Epoch 226: Loss: 405.3409\n",
            "Epoch 227: Loss: 404.7858\n",
            "Epoch 228: Loss: 404.2322\n",
            "Epoch 229: Loss: 403.6800\n",
            "Epoch 230: Loss: 403.1292\n",
            "Epoch 231: Loss: 402.5798\n",
            "Epoch 232: Loss: 402.0318\n",
            "Epoch 233: Loss: 401.4852\n",
            "Epoch 234: Loss: 400.9399\n",
            "Epoch 235: Loss: 400.3961\n",
            "Epoch 236: Loss: 399.8537\n",
            "Epoch 237: Loss: 399.3126\n",
            "Epoch 238: Loss: 398.7729\n",
            "Epoch 239: Loss: 398.2346\n",
            "Epoch 240: Loss: 397.6978\n",
            "Epoch 241: Loss: 397.1622\n",
            "Epoch 242: Loss: 396.6280\n",
            "Epoch 243: Loss: 396.0952\n",
            "Epoch 244: Loss: 395.5638\n",
            "Epoch 245: Loss: 395.0336\n",
            "Epoch 246: Loss: 394.5049\n",
            "Epoch 247: Loss: 393.9774\n",
            "Epoch 248: Loss: 393.4514\n",
            "Epoch 249: Loss: 392.9267\n",
            "Epoch 250: Loss: 392.4033\n",
            "Epoch 251: Loss: 391.8812\n",
            "Epoch 252: Loss: 391.3605\n",
            "Epoch 253: Loss: 390.8411\n",
            "Epoch 254: Loss: 390.3230\n",
            "Epoch 255: Loss: 389.8063\n",
            "Epoch 256: Loss: 389.2910\n",
            "Epoch 257: Loss: 388.7768\n",
            "Epoch 258: Loss: 388.2640\n",
            "Epoch 259: Loss: 387.7526\n",
            "Epoch 260: Loss: 387.2424\n",
            "Epoch 261: Loss: 386.7335\n",
            "Epoch 262: Loss: 386.2260\n",
            "Epoch 263: Loss: 385.7196\n",
            "Epoch 264: Loss: 385.2146\n",
            "Epoch 265: Loss: 384.7109\n",
            "Epoch 266: Loss: 384.2085\n",
            "Epoch 267: Loss: 383.7073\n",
            "Epoch 268: Loss: 383.2076\n",
            "Epoch 269: Loss: 382.7089\n",
            "Epoch 270: Loss: 382.2116\n",
            "Epoch 271: Loss: 381.7156\n",
            "Epoch 272: Loss: 381.2208\n",
            "Epoch 273: Loss: 380.7272\n",
            "Epoch 274: Loss: 380.2350\n",
            "Epoch 275: Loss: 379.7440\n",
            "Epoch 276: Loss: 379.2542\n",
            "Epoch 277: Loss: 378.7657\n",
            "Epoch 278: Loss: 378.2784\n",
            "Epoch 279: Loss: 377.7924\n",
            "Epoch 280: Loss: 377.3076\n",
            "Epoch 281: Loss: 376.8240\n",
            "Epoch 282: Loss: 376.3417\n",
            "Epoch 283: Loss: 375.8607\n",
            "Epoch 284: Loss: 375.3807\n",
            "Epoch 285: Loss: 374.9021\n",
            "Epoch 286: Loss: 374.4247\n",
            "Epoch 287: Loss: 373.9484\n",
            "Epoch 288: Loss: 373.4735\n",
            "Epoch 289: Loss: 372.9996\n",
            "Epoch 290: Loss: 372.5270\n",
            "Epoch 291: Loss: 372.0557\n",
            "Epoch 292: Loss: 371.5854\n",
            "Epoch 293: Loss: 371.1164\n",
            "Epoch 294: Loss: 370.6487\n",
            "Epoch 295: Loss: 370.1821\n",
            "Epoch 296: Loss: 369.7166\n",
            "Epoch 297: Loss: 369.2524\n",
            "Epoch 298: Loss: 368.7893\n",
            "Epoch 299: Loss: 368.3274\n",
            "Epoch 300: Loss: 367.8667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Buil in Function"
      ],
      "metadata": {
        "id": "EgYr6qVkAUor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data # input feature\n",
        "y = housing.target # target\n",
        "\n",
        "# normalization\n",
        "x_mean = np.mean(x, axis = 0)\n",
        "x_std = np.std(x, axis = 0)\n",
        "\n",
        "x_norm = (x-x_mean)/x_std\n",
        "\n",
        "# converting numpy array to tensors\n",
        "\n",
        "input = torch.from_numpy(x_norm).float()\n",
        "target = torch.from_numpy(y).float().unsqueeze(1)\n",
        "\n",
        "train_dataset = TensorDataset(input, target)\n",
        "\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(train_dataset, batch_size, shuffle = True)\n",
        "\n",
        "\n",
        "# Define the model, loss function, and optimizer\n",
        "model = nn.Linear(8,1)\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.008)\n",
        "\n",
        "\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    for xb, yb in train_dataloader:\n",
        "        #forward pass\n",
        "        prediction = model(xb)\n",
        "\n",
        "        # loss\n",
        "        loss = loss_fn(prediction, yb)\n",
        "\n",
        "        # computing gradient\n",
        "        loss.backward()\n",
        "\n",
        "        #update\n",
        "        optimizer.step()\n",
        "\n",
        "        #make gradient zer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    print(f\"Epochs: {epoch +1} Loss : {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN_fRyN1AUNq",
        "outputId": "64742dde-1510-41a9-e254-cbda07700b80"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 Loss : 0.5160\n",
            "Epochs: 2 Loss : 0.3911\n",
            "Epochs: 3 Loss : 0.4226\n",
            "Epochs: 4 Loss : 0.7277\n",
            "Epochs: 5 Loss : 0.5661\n",
            "Epochs: 6 Loss : 0.3912\n",
            "Epochs: 7 Loss : 0.5777\n",
            "Epochs: 8 Loss : 0.2013\n",
            "Epochs: 9 Loss : 0.2486\n",
            "Epochs: 10 Loss : 0.2758\n"
          ]
        }
      ]
    }
  ]
}